# Test Scenario #9: Stuck in Build Phase - TEST REPORT

**Date**: November 5, 2025
**Tester**: Ken
**Outcomist Version**: v4.1
**Result**: âœ… **SUCCESS** - User reached actionable outcome
**Time**: ~20 minutes

---

## Scenario Details

**Original Problem Statement**:
> "I'm trying to complete my Workspaces app but keep hitting LLM limitations while vibe coding. I can't get to the review phase, always stuck in building phase. I need to scrub the docs and accelerate my process."

**User Context**:
- Non-technical (no-code constraint, uses LLMs to "vibe code")
- Has 179 user stories across 4 epics (6-8 month timeline)
- Cannot get ONE flow to reviewable demo state
- Has working infrastructure but no demo to show stakeholders
- Needs to "fail fast" on product validation

---

## Test Results

### âœ… Tier 1: Pattern Recognition (30 seconds)

**What Was Tested**:
- [ ] Pattern recognition feels accurate and relevant
- [ ] Language is varied (not robotic repetition)
- [ ] Honest inference clearly marked
- [ ] Call-to-action is clear
- [ ] Immediate value delivered

**Actual Response**:
- âœ… Identified user was framing as "how to accelerate" when real question was scope management
- âœ… Reframed from "LLM limitations" to "workflow problem"
- âœ… Created hunger for deeper exploration
- âœ… Intent check (new in v4.1) shaped entire Tier 2

**What Worked**:
- Pattern recognition identified deeper issue (not surface problem)
- Intent check ("validation" vs "execution") shaped discovery appropriately
- User engaged and ready for Tier 2

**What Could Improve**:
- ðŸŸ¡ Could have been more specific after seeing docs: "You have 179 stories and can't get ONE to reviewable state. That's not an LLM limitation, that's a scope problem."

**Score**: â­â­â­â­ (4/5)

---

### âœ… Tier 2: Discovery & Recommendation (20 minutes)

#### Round 1: Landscape Questions

**Questions Asked** (3 questions):
1. "Show me the docs that need scrubbing"
2. "What are the LLM limitations you're hitting?" (example requested)
3. "What does 'complete' look like for Projects epic demo?"

**What Worked**:
- âœ… "Show me" questions â†’ saw actual docs (179 stories, complex epic structure)
- âœ… Seeing artifacts more valuable than hearing descriptions
- âœ… Questions were LOW EFFORT to answer

**Insights Discovered**:
- 179 user stories across 4 epics
- Claims "production-ready" but can't get to review phase
- Gets stuck on technical details vs showing working product

**Score**: â­â­â­â­â­ (5/5)

---

#### Round 2: Targeted Questions

**Questions Asked** (2 questions):
1. "What currently EXISTS in codebase for Projects?"
2. "What's actually BROKEN right now?"

**What Worked**:
- âœ… Dug into what matters based on Round 1 answers
- âœ… Surfaced real blocker: no reviewable demo, not "broken" code
- âœ… october-app has infrastructure but stuck in build phase

**What Could Improve**:
- ðŸŸ¡ Could have been more insistent on evidence: "Show me a recent error you had to fix"
- ðŸŸ¡ Could have probed workflow earlier (mentioned "vibe coding" in first message)

**Score**: â­â­â­â­ (4/5)

---

#### Summary Step

**What Was Said**:
Listed: Constraint (no-code), Goal (reviewable product), Problem (stuck in build), Existing state (179 stories, partial implementation)

**User Confirmation**:
"yes but 6-8 months is vibed and assumes human dev time"

**What Worked**:
- âœ… Summary separated listening from advocating
- âœ… User confirmed accuracy and added clarification
- âœ… NO JUDGMENT - just facts
- âœ… Bullet points used

**Score**: â­â­â­â­â­ (5/5)

---

#### Recommendation

**What Was Recommended**:
- Strategy: Ruthless scope cut + demo-first approach
- Define ONE hero flow (7 steps, not 179 stories)
- Fake everything not in hero flow
- Ask LLM to build "demo" not "features"
- Success = 2-min screencast, not "production-ready"

**What Worked**:
- âœ… Specific, actionable (not abstract "cut scope")
- âœ… Concrete artifact: DEMO_V1.md with demo-first strategy
- âœ… Explained WHY this works with LLM vibe coding
- âœ… Referenced user's specific context

**What Could Improve**:
- ðŸŸ¡ Could have explicitly offered co-creation upfront (user had to ask)

**Score**: â­â­â­â­â­ (5/5)

---

#### Co-Creation (Bonus)

**What Happened**:
- User asked: "Can you help me create DEMO_V1.md together?"
- Outcomist supported collaborative building of artifact
- Created executable spec file: `/Users/ken/amplifier/workspaces/_work/ken/DEMO_V1.md`

**Outcome**:
- âœ… User has concrete next step to hand to LLM
- âœ… Clear "done" definition (2-min screencast)
- âœ… Shift from "build 179 stories" to "prove one flow works"

**Score**: â­â­â­â­â­ (5/5) - Exceptional collaboration

---

## Overall Test Scores

| Phase | Score | Notes |
|-------|-------|-------|
| Tier 1 Pattern Recognition | â­â­â­â­ (4/5) | Could be more specific with numbers |
| Round 1 Questions | â­â­â­â­â­ (5/5) | "Show me" questions worked perfectly |
| Round 2 Questions | â­â­â­â­ (4/5) | Could probe workflow earlier |
| Summary | â­â­â­â­â­ (5/5) | Perfect execution |
| Recommendation | â­â­â­â­â­ (5/5) | Specific and actionable |
| Co-Creation | â­â­â­â­â­ (5/5) | Exceptional (user-initiated) |
| **OVERALL** | **â­â­â­â­â­ (4.7/5)** | **Excellent** |

---

## New Patterns Identified

### "Stuck in Build Phase" User Profile

**Characteristics**:
- Ambitious product vision (multi-epic roadmap)
- Can't get to reviewable state (demo, prototype, video)
- Blame tools/process but real issue is scope
- Have infrastructure but no end-to-end flow working
- Need permission to ruthlessly cut scope

**What They Need**:
1. **Scope reality check** - "You're building 179 things when you need 1 thing"
2. **Concrete 'done' definition** - "Demo-ready = 2-min screencast"
3. **Permission to fake it** - "Hardcode everything not in hero flow"
4. **Executable artifact** - Not just advice, but DEMO_V1.md they can hand to LLM

### Why This Pattern Emerges with No-Code/LLM Builders

Users who don't code themselves:
- Rely on LLM to build everything
- LLMs are GREAT at: copying patterns, hardcoding demos, vertical slices
- LLMs are TERRIBLE at: architecture, anticipating edge cases, knowing "done"
- Users ask for "features" â†’ LLM builds infrastructure â†’ never reaches reviewable state
- Users need to ask for "demo" â†’ LLM builds visible UI â†’ can actually review

---

## Improvements Incorporated Into Shared Prompts

Based on Ken's feedback, we added the following to `outcomist_shared/prompts.py`:

### âœ… 1. Evidence Gathering ("Show Me")
**Added to Round 1**:
```
EVIDENCE GATHERING (Ken's feedback - Test #9):
- When user describes problems, errors, docs, or artifacts â†’ ask to see them
- "Show me" is better than "Tell me" when evidence exists
- Don't accept descriptions when actual artifacts are available
```

### âœ… 2. Reviewable Artifact Probe
**Added to Round 1**:
```
SPECIAL PROBES (based on context):
- If building/creating something â†’ Ask early: "Do you have something reviewable
  right now (demo, video, screenshot, prototype) or is it all in-progress?"
```

### âœ… 3. Workflow/Constraint Probing
**Added to Round 1**:
```
- If mentions constraints (no-code, solo, non-technical) â†’ Ask: "Walk me through
  your typical workflow when you [do this]. Where does it usually break down?"
```

### âœ… 4. Explicit Co-Creation Offer
**Added to Recommendation**:
```
CO-CREATION OFFER (Ken's feedback - Test #9):
After delivering recommendation, explicitly offer collaboration:
"Want me to help you create [the artifact/next step] together, or do you want
to take it from here?"
```

### âœ… 5. Time Expectation Setting
**Added as comment in shared prompts**:
```
# Note: Time expectation should be set in the frontend when offering Tier 2
# Recommended text: "This will take about 10 minutes with 4-5 questions. If your
# problem is simpler, let me know and I can give you a quick take instead."
```

---

## User Feedback

### Immediate Reaction
> "this is awesome, let's summarize this session and help Chris build a better outcomist"

> "I think this flow helped me focus my work in collaborating with my PM! I really plan on using outcomist itself for help me think through some side software projects and even my real estate projects"

### Feature Request #1: Persistent Context Across Sessions
> "I would *love* this, if it can learn about me more over time and help me think through problems with previous knowledge"

User wants Outcomist to:
- Remember past decisions and patterns
- Reference previous problems when exploring new ones
- Become personalized advisor that knows constraints, preferences, history
- Think: "Last time we worked on X, you prioritized Y - is that still true?"

### Feature Request #2: Dedicated Onboarding Command
> "it might be helpful in the future with another command maybe that outcomist can actually get to know me and my background in general"

Suggested two-command pattern:
1. **`/profile`** (one-time) - "Tell me about yourself so I can help you better"
2. **`/explore`** (recurring) - "Let's think through this decision" (leverages profile)

---

## Comparison to Test Suite

### Similar Scenarios
- **Scenario #11**: Client vs Product (similar scope management)
- **Scenario #13**: Business uncertainty + resource constraints
- **Scenario #16**: Crisis decision + sunk cost fallacy

### Unique Aspects of This Test
- âœ… First test with **no-code/LLM builder** user type
- âœ… First test of **co-creation mode** (user asked for help creating artifact)
- âœ… First test of **intent check** feature (new in v4.1)
- âœ… Tests "Show Me" questions with real files/docs
- âœ… Validates **summary before recommendation** approach

### Why This Test Matters
- Identifies new user persona (no-code builders)
- Tests co-creation capabilities
- Validates v4.1 improvements (intent check)
- Surfaces need for persistent context (feature request)
- Demonstrates high repeat-usage intent

---

## Metrics

| Metric | Value |
|--------|-------|
| Time to Actionable Outcome | ~20 minutes |
| Questions Asked | 5 across 2 rounds |
| User Engagement | High (stayed engaged, asked for co-creation) |
| Recommendation Specificity | Very High (concrete artifact created) |
| User Satisfaction | High ("yup this is awesome") |
| Repeat Usage Intent | Very High (plans multiple use cases) |

---

## Test Case Classification

**Scenario Type**: Product/Project
**Stakes**: High
**Complexity**: High
**User Type**: Non-technical (LLM builder)
**Decision Type**: Execution (not validation)
**Time Pressure**: Medium (not urgent but frustrated)
**Constraint**: No-code (non-negotiable)

**Test Category**: Real-world validation âœ…
**Recommended for Regression**: Yes
**Priority**: High (tests new user persona + co-creation)

---

## Recommended Actions

### Immediate
- [x] Incorporate Ken's feedback into shared prompt library
- [x] Document test scenario in standard format
- [ ] Update frontend to show time expectation when offering Tier 2

### Short-term
- [ ] Test revised prompts with another "Stuck in Build Phase" scenario
- [ ] Create formal "Build Mode" for co-creation workflows
- [ ] Add to automated test suite (if possible)

### Long-term
- [ ] Implement persistent context across sessions (Feature Request #1)
- [ ] Create `/profile` command for onboarding (Feature Request #2)
- [ ] Build library of common artifacts (DEMO_V1.md template, etc.)

---

## Conclusion

**Test Verdict**: âœ… **PASS WITH EXCELLENCE**

Scenario #9 validates that Outcomist v4.1:
- âœ… Effectively handles non-technical users with unique constraints
- âœ… Identifies deeper patterns beyond surface problem framing
- âœ… Delivers specific, actionable recommendations
- âœ… Supports co-creation when user needs concrete artifacts
- âœ… Creates high repeat-usage intent

**Key Learnings**:
1. "Show me" questions reveal more than "tell me" questions
2. Non-technical users need concrete artifacts, not just advice
3. Co-creation mode is valuable (should be offered explicitly)
4. Time expectations matter (user didn't know how long it would take)
5. Persistent context would significantly increase value

**Improvements Made**: All 5 of Ken's suggestions incorporated into shared prompt library.

**Overall Assessment**: This test case reveals a valuable new user persona (no-code/LLM builders) and validates the importance of moving beyond advice to artifact creation. The session resulted in an executable next step, not just guidance.

---

**Test conducted by**: Ken
**Documented by**: Chris
**Date**: November 5, 2025
**Outcomist Version**: v4.1 â†’ v4.2 (with improvements)
