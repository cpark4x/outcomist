# Epic 2: Proposal Review

**Status**: ⏳ Proposed (not yet implemented)
**Duration**: Variable (depends on proposal complexity)
**Goal**: Get user to "Yes, this is exactly what I want"

---

## Overview

Epic 2 is where users review the AI's tangible, high-quality proposal. The proposal isn't a high-level plan—it's a complete project deliverable with enough polish that users can confidently evaluate whether it's what they want.

**Critical insight**: Users want to get to Epic 2 as quickly as possible. Epic 1 exists solely to make Epic 2 proposals high-quality, executable, and gap-free enough for confident evaluation.

**Quality is the #1 goal.**

---

## User Story

"As a decision-maker, I need to review a tangible, polished proposal with working prototypes or complete artifacts, so I can confidently say 'yes, this is exactly what I want' before committing to execution."

---

## Core Principles

### Outcome-Oriented Proposals
Not "here's what we'll do" but "here's what you'll have at the end"

**Bad**: "We'll build a web dashboard with tracking features"
**Good**: "Here's a working prototype you can click through. Every screen, every button, every interaction. This is exactly what you'll get."

Focus on the end state, make it tangible, provide enough polish to evaluate quality.

### No "Floating Countertop" Proposals
AI only proposes what it can **actually execute** within user's constraints.

**Bad**: AI proposes custom ML recommendation engine (user can't deploy ML, no training data, too complex)
**Good**: AI proposes Shopify's built-in filtering with custom tags (AI can write templates, user can manage in admin)

Discovery (Epic 1) must validate executability before reaching Epic 2.

### Quality Over Speed
Epic 2 proposals should have enough quality that users can confidently evaluate them.

What "quality" means depends on project type:
- **Software**: Working prototype (clickable, functional)
- **Content**: Polished drafts (ready to publish or near-ready)
- **Process**: Complete templates with real examples
- **Documents**: Fully working artifacts (can use immediately)
- **Strategy**: Complete documents with clear recommendations

### Context-Dependent Excellence
"What would an expert agency do for this topic?"

- Software agency → Working prototype
- Strategy consultant → Complete deliverables
- Content agency → Finished examples
- Process consultant → Templates + real examples

Match the standard users expect from domain experts.

### Iterative Review Process
Walk through proposal, gather feedback along the way. Depends on project type:
- Software: Click through prototype, comment on screens
- Content: Read pieces, comment inline
- Process: Walk through template, test with scenarios
- Documents: Use with real data, identify gaps
- Strategy: Review thinking, challenge assumptions

Not one big review at the end—progressive refinement.

---

## What AI Delivers in Epic 2

A complete project proposal with these sections:

### Section 1: What You'll Get (Tangible Outcome)

Not abstract descriptions—concrete, evaluable artifacts.

**Software projects**:
- Working prototype (deployed, clickable)
- All screens implemented
- All interactions working
- Real or realistic demo data
- Proof AI can build this

**Content projects**:
- Finished drafts (polished, ready to use)
- Examples showing range and quality
- Style/tone demonstrated

**Process projects**:
- Complete templates (ready to use)
- Real examples (5+ scenarios)
- Before/after comparisons

**Document/Artifact projects**:
- Fully working documents (can use immediately)
- Populated with realistic data
- Instructions for customization

**Strategy projects**:
- Complete strategy document
- Clear recommendations
- Supporting analysis/research
- Actionable next steps

**Key**: Enough polish that user can evaluate "is this what I want?" with confidence.

### Section 2: What It Takes

**Your commitment**:
- Specific actions required from user
- Time investment (hours/week, duration)
- Decisions user needs to make
- Training/learning required

**Resources needed**:
- Budget/costs (itemized breakdown)
- Tools/services required
- People/skills needed
- Infrastructure requirements

**Timeline**:
- Milestones with dates
- Critical path items
- When you'll see results
- Dependencies and blockers

### Section 3: Explicitly Identified Gaps/Holes

"This proposal covers X, Y, Z. It does NOT include A, B, C."

Be honest about:
- Features not included in this scope
- Assumptions being made
- Risks or unknowns
- Optional enhancements available

### Section 4: Options & Trade-offs

**Core proposal**: [The main recommendation]

**Optional add-ons**:
- Enhancement A: [What + Why + Cost/Time]
- Enhancement B: [What + Why + Cost/Time]

**Alternative approaches**:
- Alternative 1: [Different path with trade-offs]
- Alternative 2: [Different path with trade-offs]

**Trade-offs made**:
- "I'm recommending X over Y because: [reasoning]"
- "This optimizes for [priority] at the expense of [other priority]"

---

## Review Process

### Iterative Walkthrough

The review format depends on project type:

**Software projects**:
1. Click through working prototype
2. Test interactions and flows
3. Comment on each screen
4. Identify gaps or issues
5. Discuss optional enhancements
6. Refine based on feedback
7. Re-present refined prototype
8. Iterate until: "Yes, this is exactly what I want"

**Content projects**:
1. Read first piece together
2. Comment inline on tone, accuracy, quality
3. Review next pieces
4. Discuss overall direction
5. Refine based on feedback
6. Re-present polished versions
7. Iterate until: "Yes, publish these"

**Process projects**:
1. Walk through template structure
2. Test with 3-5 real scenarios
3. Identify gaps or awkward parts
4. Discuss adaptations needed
5. Refine template
6. Test again with new scenarios
7. Iterate until: "Yes, we can roll this out"

**Document/Artifact projects**:
1. Use document with real data
2. Identify what works and what doesn't
3. Discuss missing elements
4. Refine document
5. Test with another dataset
6. Iterate until: "Yes, this is exactly what I need"

**Strategy projects**:
1. Review executive summary
2. Challenge assumptions and logic
3. Discuss supporting research
4. Question recommendations
5. Refine thinking based on feedback
6. Re-present refined strategy
7. Iterate until: "Yes, let's execute this"

### Key Questions During Review

- "Does this outcome match your vision?"
- "Is the timeline realistic?"
- "Any constraints I'm missing?"
- "Which options interest you?"
- "What concerns you?"
- "What would make this proposal better?"

---

## Output of Epic 2

User says: **"Yes, this is the right plan. I'm ready to proceed."**

Proposal is locked and ready for execution (Epic 3).

---

## Winning Scenarios (First 5)

**Strategy**: Win a few scenarios first, not all. Pick 5 where we can demonstrate clear value and AI can execute end-to-end.

### Scenario 1: Simple Web Feature/Tool

**Examples**: Order tracking page, contact form, pricing calculator, booking widget

**Why this wins**:
- AI can build working prototype
- User can click through and evaluate
- Clear success criteria
- Deployable outcome
- High value if done well

**Epic 2 proposal format**:
- Live working prototype (deployed, clickable)
- Complete code repository
- Deployment instructions
- Service configuration details
- Cost breakdown

**Quality bar**: Working prototype with enough polish to evaluate UX and functionality

---

### Scenario 2: Content Package

**Examples**: Blog post series, email sequence, social media campaign, newsletter content

**Why this wins**:
- AI can write complete drafts
- User can read and evaluate quality
- Clear quality bar
- Immediately usable
- High-value if content is good

**Epic 2 proposal format**:
- Finished content drafts (polished, near-ready to publish)
- Editorial calendar
- SEO optimization included
- Distribution strategy
- Performance tracking plan

**Quality bar**: Polished drafts that need minimal editing before publication

---

### Scenario 3: Process/Meeting Structure

**Examples**: Decision framework, weekly sync structure, project intake process, review workflow

**Why this wins**:
- AI can create complete templates
- User can see real examples
- Clear before/after comparison
- Can start using immediately
- High value if process improves efficiency

**Epic 2 proposal format**:
- Complete templates (meeting agendas, decision logs, workflows)
- 5+ real examples demonstrating use
- Before/after process comparison
- Training materials for team
- Rollout plan

**Quality bar**: Templates complete enough to use immediately with real scenarios

---

### Scenario 4: Document/Artifact Creation

**Examples**: Proposal template, pricing calculator, project plan, client onboarding doc

**Why this wins**:
- AI can build working document/tool
- User can test with real data
- Clear utility
- Immediately useful
- High value if well-designed

**Epic 2 proposal format**:
- Fully working document or tool
- Populated with realistic demo data
- Instructions for customization
- Template for reuse
- Maintenance guide

**Quality bar**: Working artifact that user can immediately use with their real data

---

### Scenario 5: Strategy/Plan Document

**Examples**: Go-to-market plan, pricing strategy, content strategy, growth plan

**Why this wins**:
- AI can draft complete document
- User can evaluate thinking quality
- Clear recommendations
- Actionable next steps
- High value if strategy is sound

**Epic 2 proposal format**:
- Complete strategy document (10-20 pages)
- Executive summary
- Supporting research/analysis
- Clear recommendations
- Implementation roadmap
- Success metrics

**Quality bar**: Complete document with depth of thinking that demonstrates expertise

---

## Quality Standards

### What Makes a Good Proposal

**✅ Good proposal**:
- Tangible outcome user can evaluate
- Enough polish to assess quality confidently
- Explicitly identifies gaps and assumptions
- Shows AI can actually execute this
- Clear on what's included and what's not
- Options and trade-offs transparent
- Timeline and resources realistic

**❌ Bad proposal**:
- High-level description without tangible artifacts
- "Floating countertop" solutions AI can't execute
- Gaps not identified (surprises during execution)
- Unrealistic timelines or resources
- Unclear what's included vs optional
- No alternatives or trade-offs discussed

### Refinement Cycles

**Expected**: 1-3 refinement cycles before "yes"
- Cycle 1: Initial proposal, user identifies issues/gaps
- Cycle 2: Refined proposal addressing feedback
- Cycle 3 (if needed): Final refinements

**If more than 3 cycles**: Something went wrong in Epic 1 (discovery was insufficient)

---

## Transition to Epic 3

### When Ready
After Epic 2, user has:
- ✅ Reviewed tangible, polished proposal
- ✅ Provided feedback and seen refinements
- ✅ Said "Yes, this is exactly what I want"
- ✅ Validated proposal is ready for execution

### User Choice
- **Continue to Epic 3**: Execute the validated proposal with AI support
- **Execute independently**: Take proposal and build yourself
- **Pause**: Proposal validated but not ready to execute yet

### What Flows Forward to Epic 3
- Validated proposal (locked)
- All refinements incorporated
- Clear success criteria
- Timeline and milestones
- Resource requirements

**Note**: Production deployment happens in Epic 3, not Epic 2.

**Why separate deployment from proposal?**
- Epic 2 focus: Get user to "yes"
- Allow refinement without production concerns
- User commits before deployment
- Can iterate proposal without production issues

---

## Open Questions

### For Each Winning Scenario

**1. Exact quality bar definition**
- How complete is "complete enough"?
- How polished is "polished enough"?
- What gaps are acceptable to leave?

**2. Review process specifics**
- What does iterative review look like?
- How many refinement rounds?
- What feedback mechanisms?

**3. Production deployment approach**
- Does deployment happen in Epic 2 or Epic 3?
- What's the handoff process?
- Who maintains after deployment?

**4. Success criteria**
- How do we know Epic 2 proposal is "good enough"?
- What metrics validate quality?
- How do we measure user satisfaction?

### General Product Questions

**1. Discovery process refinement**
- What questions reliably validate executability?
- How to identify "floating countertop" proposals early?
- When to push back vs adapt solution?

**2. Proposal templates**
- Should we have templates per scenario type?
- How much standardization vs customization?
- What format works best for review?

**3. Epic transitions**
- How to guide users from Epic 1 → Epic 2?
- What signals indicate readiness for Epic 3?
- Can users skip Epic 3 and execute independently?

---

## Success Metrics

### Qualitative
- ✅ User can confidently evaluate proposal quality
- ✅ No major surprises during execution
- ✅ Gaps/holes explicitly identified upfront
- ✅ User says "Yes, this is exactly what I want"

### Quantitative
- ✅ 80%+ proposals accepted within 3 refinement cycles
- ✅ <10% proposals need major rework after acceptance
- ✅ 0% proposals include non-executable solutions
- ✅ User satisfaction score 8+/10

---

## Future Implementation

Will follow same pattern as Epic 1:
- Module for orchestration (ProposalEngine)
- Streaming interactions where appropriate
- Session-based state management
- Prompts in `outcomist_shared/prompts.py`
- Web interface integration

**Next step**: Pick ONE winning scenario to build first, validate the Epic 2 approach end-to-end, then expand to other scenarios.

---

## Related Documentation

- [Product Vision v2](../vision/product-vision-v2.md) - Strategic context
- [End-to-End Flow](../vision/end-to-end-flow.md) - Complete journey
- [Epic 1: Deep Discovery](./epic-1-deep-discovery.md) - Discovery process that feeds into proposals

---

## Document History

**Created**: 2025-01-10
**Replaces**: epic-4-design.md (renamed and refocused)
**Status**: Aligned with Product Vision v2
**Audience**: Product team, AI assistants, future builders
